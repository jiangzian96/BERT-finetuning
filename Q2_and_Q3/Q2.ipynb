{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "class",
      "language": "python",
      "name": "class"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Q2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5qppgJHnAZm",
        "colab_type": "text"
      },
      "source": [
        "---   \n",
        "# HW3 - Transfer learning\n",
        "\n",
        "#### Due October 30, 2019\n",
        "\n",
        "In this assignment you will learn about transfer learning. This technique is perhaps one of the most important techniques for industry. When a problem you want to solve does not have enough data, we use a different (larger) dataset to learn representations which can help us solve our task using the smaller task.\n",
        "\n",
        "The general steps to transfer learning are as follows:\n",
        "\n",
        "1. Find a huge dataset with similar characteristics to the problem you are interested in.\n",
        "2. Choose a model powerful enough to extract meaningful representations from the huge dataset.\n",
        "3. Train this model on the huge dataset.\n",
        "4. Use this model to train on the smaller dataset.\n",
        "\n",
        "\n",
        "### This homework has the following sections:\n",
        "1. Question 1: MNIST fine-tuning (Parts A, B, C, D).\n",
        "2. Question 2: Pretrain on Wikitext2 (Part A, B, C, D)\n",
        "3. Question 3: Finetune on MNLI (Part A, B, C, D)\n",
        "4. Question 4: Finetune using pretrained BERT (Part A, B, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902GO7UlnAaK",
        "colab_type": "text"
      },
      "source": [
        "--- \n",
        "# Question 2 (train a model on Wikitext-2)\n",
        "\n",
        "Here we'll apply what we just learned to NLP. In this section we'll make our own feature extractor and pretrain it on Wikitext-2.\n",
        "\n",
        "The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License.\n",
        "\n",
        "#### Part A\n",
        "In this section you need to generate the training, validation and test split. Feel free to use code from your previous lectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz-jicw-owto",
        "colab_type": "code",
        "outputId": "b6962686-7390-4fc9-aad7-8fd6abe9f295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install jsonlines\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import spacy\n",
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAlakqEMrany",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "        # include test\n",
        "        for line in tqdm(datasets['test']):\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "        \n",
        "        \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piEsYWjDsZ_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_dataset_wikitext(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx8hPYUHnAaL",
        "colab_type": "code",
        "outputId": "2fd20194-9924-4b16-cac8-6fbf0ca69dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "      if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "      datasets = json.load(open(filename, 'r'))\n",
        "      for name in datasets:\n",
        "          datasets[name] = [x.split() for x in datasets[name]]\n",
        "      vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "      print(\"Vocab size: %d\" % (len(vocab)))\n",
        "      return datasets, vocab\n",
        "    \n",
        "datasets,vocab = load_wikitext()\n",
        "wikitext_dict = Dictionary(datasets, include_valid=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 19:37:04--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 107.152.26.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|107.152.26.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-15 19:37:04--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-15 19:37:04--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 107.152.26.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|107.152.26.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!ZicsbP3pfz37kmpwtatD6-ggThmfls0FTdlsB0ps_jkmOs2Q-BDtal9_ZIj7FCmjwZdf-D9bx-ugHpf_k1lnjXCzGMgabYwnAqVs_AZchiznICSLLVgaz1QvG1gUZDh4WphL97MdmfQWvjpuETmiMNgjpogt9eK68FHigVEwt_KaO7GR_YjayWQPrijm7F-JHgS5HGiuKnfAV-CnP-OrRTyW06E1EAzuTRRS9BQaAOtjYuSPiYaBy4Kvvv6Z4fk8udjPDvffRX4N-YfdBCWDKyCRdCFc6mS5WVrm6dZ52-uZUm5Jbq38xWKROdzWDxxWB3uU1mA-9pi8iCZzIzN6wuPoUC1OEpI6Q4qevLoJIsZA3FIT1-faKeE36iRdTBLAqDk6HmyIKsPmYtw7mAJkrlNDdiNN1jpSoWxDG2Rt2EGm82NQr72kuXG9JO1xJ9rOnWL-qtKNVm8ZhVyywYeRqY10YU1LA5DOfpHcAQocS8s1Kp-D9SUkbqoz14tFmJDDylEJBlx_VkkDue6ojfSt5kX7vR1srjiJhvBD6ha3fuM20Sjhrug1G0ENDgq609P_ta1D8-_m6llKTVqyz7WtG5BJJPMiD-5FUiF70-v3AdHcNk4763W5DFYdmVndUKfKYYPM-iQiejhsf7YNl6O0LLrJCz8rtalXZMgXxHiKWWZoulOsBUV6F6lEDQsWttGnYLJlCcPJE10KDdYUx3JrTfyz5Mr_wQs7-9DAaDqMbwTGF7njrNn4kjdcuUQ4YapPhzwDJjgPaQU2v8YdU6U9DQMIAnG60DyYLkSpYaW3ZRI_w29Nrm3FesNQ1EjL0JqdIswJtZ26q8cZde5s21boSPpcaiI_HnVLTFogjD4Pxu3sljJeB8AGv3UdLnf353KM4jPCU2XLqZ_cQde8BGMaH0oFOjmdJZu3WUhMrACTso7jSNynmT7JdEnmXch5XN727kaclvRiNscmIu9uAlNB_KuWiMkwavPOuQDQTSFKU9iKb4vONRjd5XQFGuMCfkVIKC-r9tF69iSQc0XDPTNLCZ_Wf2rjaG2yuc-X3OZgI1E1t-xUdKSZCnvfykx1dni3RAy6W3DFv1V1CTzjM-FQimsaxn6PhtxN3sqZiMmVRjMLeH7A2dHoSltJejbozg0Ue33__xt7r5Wsvy1H2auaZCl3wNkaUfWfCxw8XwUnoIYHMbkAN8WK4Z9IwZScRwpbLOezsx4FuWFnya3Apk5H-iinvfPfQH8QnMxfyqunu0pbq53FCb0v-QuGATCxRkA-QkUSVSLCvwV76nrELfQbj2OSFWZZQvfcUtQt4BQLK7UvnY-lVL4vy0m997uprLlSky-1wZ3X45BIGEjnUok4ZUYmSE7tRFvQxYHm6K3ce0ALGDytal0GKko6zNDe8H0ZjI3G824KRjyiE6IS/download [following]\n",
            "--2019-10-15 19:37:05--  https://public.boxcloud.com/d/1/b1!ZicsbP3pfz37kmpwtatD6-ggThmfls0FTdlsB0ps_jkmOs2Q-BDtal9_ZIj7FCmjwZdf-D9bx-ugHpf_k1lnjXCzGMgabYwnAqVs_AZchiznICSLLVgaz1QvG1gUZDh4WphL97MdmfQWvjpuETmiMNgjpogt9eK68FHigVEwt_KaO7GR_YjayWQPrijm7F-JHgS5HGiuKnfAV-CnP-OrRTyW06E1EAzuTRRS9BQaAOtjYuSPiYaBy4Kvvv6Z4fk8udjPDvffRX4N-YfdBCWDKyCRdCFc6mS5WVrm6dZ52-uZUm5Jbq38xWKROdzWDxxWB3uU1mA-9pi8iCZzIzN6wuPoUC1OEpI6Q4qevLoJIsZA3FIT1-faKeE36iRdTBLAqDk6HmyIKsPmYtw7mAJkrlNDdiNN1jpSoWxDG2Rt2EGm82NQr72kuXG9JO1xJ9rOnWL-qtKNVm8ZhVyywYeRqY10YU1LA5DOfpHcAQocS8s1Kp-D9SUkbqoz14tFmJDDylEJBlx_VkkDue6ojfSt5kX7vR1srjiJhvBD6ha3fuM20Sjhrug1G0ENDgq609P_ta1D8-_m6llKTVqyz7WtG5BJJPMiD-5FUiF70-v3AdHcNk4763W5DFYdmVndUKfKYYPM-iQiejhsf7YNl6O0LLrJCz8rtalXZMgXxHiKWWZoulOsBUV6F6lEDQsWttGnYLJlCcPJE10KDdYUx3JrTfyz5Mr_wQs7-9DAaDqMbwTGF7njrNn4kjdcuUQ4YapPhzwDJjgPaQU2v8YdU6U9DQMIAnG60DyYLkSpYaW3ZRI_w29Nrm3FesNQ1EjL0JqdIswJtZ26q8cZde5s21boSPpcaiI_HnVLTFogjD4Pxu3sljJeB8AGv3UdLnf353KM4jPCU2XLqZ_cQde8BGMaH0oFOjmdJZu3WUhMrACTso7jSNynmT7JdEnmXch5XN727kaclvRiNscmIu9uAlNB_KuWiMkwavPOuQDQTSFKU9iKb4vONRjd5XQFGuMCfkVIKC-r9tF69iSQc0XDPTNLCZ_Wf2rjaG2yuc-X3OZgI1E1t-xUdKSZCnvfykx1dni3RAy6W3DFv1V1CTzjM-FQimsaxn6PhtxN3sqZiMmVRjMLeH7A2dHoSltJejbozg0Ue33__xt7r5Wsvy1H2auaZCl3wNkaUfWfCxw8XwUnoIYHMbkAN8WK4Z9IwZScRwpbLOezsx4FuWFnya3Apk5H-iinvfPfQH8QnMxfyqunu0pbq53FCb0v-QuGATCxRkA-QkUSVSLCvwV76nrELfQbj2OSFWZZQvfcUtQt4BQLK7UvnY-lVL4vy0m997uprLlSky-1wZ3X45BIGEjnUok4ZUYmSE7tRFvQxYHm6K3ce0ALGDytal0GKko6zNDe8H0ZjI3G824KRjyiE6IS/download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.25.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.25.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  18.6MB/s    in 0.7s    \n",
            "\n",
            "2019-10-15 19:37:06 (18.6 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 457/78274 [00:00<00:17, 4568.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:25<00:00, 538.47it/s]\n",
            "100%|██████████| 8464/8464 [00:12<00:00, 695.00it/s]\n",
            "100%|██████████| 9708/9708 [00:12<00:00, 794.32it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dQgAJgS6fDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_wikitext_dataset(datasets):\n",
        "    \"\"\"\n",
        "    Fill in the details\n",
        "    \"\"\"\n",
        "    raw_train = datasets[\"train\"]\n",
        "    raw_val = datasets[\"valid\"]\n",
        "    raw_test = datasets[\"test\"]\n",
        "    \n",
        "    return raw_train,raw_val,raw_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgJGeP9rp2-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikitext_train,wikitext_val,wikitext_test = init_wikitext_dataset(datasets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFZRaWSwtjlz",
        "colab_type": "code",
        "outputId": "1f6b5f0a-73ec-44e1-a7e1-96a8b8302d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# checking some example\n",
        "print(' '.join(wikitext_train[3010]))\n",
        "\n",
        "encoded = wikitext_dict.encode_token_seq(wikitext_train[3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wikitext_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1YDBrhuTub5",
        "colab_type": "code",
        "outputId": "1d5cb2d2-0bbd-4f53-f0d3-9130dc24b830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33175"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT3_Qm2_nAaP",
        "colab_type": "text"
      },
      "source": [
        "#### Part B   \n",
        "Here we design our own feature extractor. In MNIST that was a resnet because we were dealing with images. Now we need to pick a model that can model sequences better. Design an RNN-based model here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUObv6uKzR1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOAD_PRETRAINED = True\n",
        "IGNORE_PROJECTION = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk9l6cQIwUP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Snbmhr0vU8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, rnn and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['lstm_dropout'], batch_first=True)\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        # |V| -> emb_dim\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        # emb_dim, hidden -> output, (h_n,c_n)     (hidden,hidden*num_layers)\n",
        "        lstm_outputs = self.lstm(embeddings)\n",
        "        logits = self.projection(lstm_outputs[0])\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7HJJBy7wbV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "if LOAD_PRETRAINED:\n",
        "  model_dict = torch.load(\"LSTM_model3.pt\")\n",
        "  model_weights = torch.load('LSTM_checkpoint3.pt')\n",
        "  options = model_dict['options']\n",
        "  model_LSTM = LSTMLanguageModel(options).to(current_device)\n",
        "  model_LSTM.load_state_dict(model_weights)\n",
        "\n",
        "else:\n",
        "  embedding_size = 256\n",
        "  hidden_size = 256\n",
        "  num_layers = 3\n",
        "  lstm_dropout = 0.3\n",
        "  options = {\n",
        "          'num_embeddings': len(wikitext_dict),\n",
        "          'embedding_dim': embedding_size,\n",
        "          'padding_idx': wikitext_dict.get_id('<pad>'),\n",
        "          'input_size': embedding_size,\n",
        "          'hidden_size': hidden_size,\n",
        "          'num_layers': num_layers,\n",
        "          'lstm_dropout': lstm_dropout,\n",
        "      }\n",
        "  model_LSTM = LSTMLanguageModel(options).to(current_device)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwrQD2NInAaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_feature_extractor(model):\n",
        "    feature_extractor = model\n",
        "    \n",
        "    return feature_extractor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdwybaIX10G-",
        "colab_type": "code",
        "outputId": "dbd4551d-eb03-47f5-da15-73db9b392772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "feature_extractor = init_feature_extractor(model_LSTM)\n",
        "feature_extractor.named_children"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.named_children of LSTMLanguageModel(\n",
              "  (lookup): Embedding(33186, 256, padding_idx=2)\n",
              "  (lstm): LSTM(256, 256, num_layers=3, batch_first=True, dropout=0.3)\n",
              "  (projection): Linear(in_features=256, out_features=33186, bias=True)\n",
              ")>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwQqU67HnAaS",
        "colab_type": "text"
      },
      "source": [
        "#### Part C\n",
        "Pretrain the feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIitXEB03H6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBmvMhNV3TBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    pad_token = wikitext_dict.get_id('<pad>')\n",
        "    #pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we_-XeSJ3WwE",
        "colab_type": "code",
        "outputId": "42df44bd-3976-4e6a-e971-ec1f7eb25bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "wikitext_tokenized_datasets = tokenize_dataset_wikitext(datasets, wikitext_dict)\n",
        "wikitext_tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in wikitext_tokenized_datasets.items():\n",
        "    wikitext_tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "wikitext_tensor_dataset['train'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 94013.99it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 120218.32it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 36790.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10, 19,\n",
              "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PIMBptq7Gdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wikitext_loaders = {}\n",
        "batch_size = 256 #64\n",
        "\n",
        "for split, wikitext_dataset in wikitext_tensor_dataset.items():\n",
        "    wikitext_loaders[split] = DataLoader(wikitext_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GGgwLmW6F_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0,name = 'LSTM_'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.name = name\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score - self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.name+'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McEbvxY86GDm",
        "colab_type": "code",
        "outputId": "9c9fdc7c-6fdd-4eea-d3ab-334593849877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "patience = 5\n",
        "early_stopping = EarlyStopping(patience=patience, verbose=True,name=\"LSTM_\")\n",
        "early_stopping"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.EarlyStopping at 0x7f733c9a4128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NStztlyi6GPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wikitext_dict.get_id('<pad>'))\n",
        "\n",
        "model_parameters = [p for p in feature_extractor.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(model_parameters, lr=0.001, momentum=0.999)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvvGvT-nAaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val):\n",
        "    # FILL IN THE DETAILS\n",
        "    plot_cache = []\n",
        "\n",
        "    for epoch_number in range(100):\n",
        "        avg_loss=0\n",
        "        if not LOAD_PRETRAINED:\n",
        "            # do train\n",
        "            feature_extractor.train()\n",
        "            train_log_cache = []\n",
        "            for i, (inp, target) in enumerate(wikitext_loaders['train']):\n",
        "                optimizer.zero_grad()\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                logits = feature_extractor(inp)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_log_cache.append(loss.item())\n",
        "\n",
        "                if i % 100 == 0:\n",
        "                    avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                    print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                    train_log_cache = []\n",
        "\n",
        "        #do valid\n",
        "        valid_losses = []\n",
        "        feature_extractor.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(wikitext_loaders['valid']):\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                logits = feature_extractor(inp)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item())\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "\n",
        "        plot_cache.append((avg_loss, avg_val_loss))\n",
        "        \n",
        "        if LOAD_PRETRAINED:\n",
        "            print(\"Validation PPL:\",2**(avg_val_loss/np.log(2)))\n",
        "            break\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejDlUwmu_aht",
        "colab_type": "code",
        "outputId": "dff73521-d37f-41ae-8ca8-248eaea7c7a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss after 0 epoch = 5.0082\n",
            "Validation PPL: 149.63181996722716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrrP-lfQnAaX",
        "colab_type": "text"
      },
      "source": [
        "#### Part D\n",
        "Calculate the test perplexity on wikitext2. Feel free to recycle code from previous assignments from this class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l4sLZP1nAaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_wiki2_test_perplexity(feature_extractor, wikitext_test):\n",
        "    \n",
        "    # FILL IN DETAILS\n",
        "    plot_cache = []\n",
        "    \n",
        "    #do valid\n",
        "    test_losses = []\n",
        "    feature_extractor.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(wikitext_loaders['test']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = feature_extractor(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            test_losses.append(loss.item())\n",
        "        avg_test_loss = sum(test_losses) / len(test_losses)\n",
        "        print('Test loss = {:.{prec}f}'.format(avg_test_loss, prec=4))\n",
        "\n",
        "    plot_cache.append(avg_test_loss)\n",
        "    test_ppl = 2**(avg_test_loss/np.log(2))   \n",
        "    print('Test PPL:', test_ppl)\n",
        "    return test_ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPIdhHoenAaa",
        "colab_type": "text"
      },
      "source": [
        "#### Let's grade your results!\n",
        "(don't touch this part)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXb0bRuonAac",
        "colab_type": "code",
        "outputId": "597632e2-2cef-4688-b8a7-1163b9d1cc0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def grade_wikitext2():\n",
        "    # load data\n",
        "    wikitext_train, wikitext_val, wikitext_test = init_wikitext_dataset(datasets)\n",
        "\n",
        "    # load feature extractor\n",
        "    feature_extractor = init_feature_extractor(model_LSTM)\n",
        "\n",
        "    # pretrain using the feature extractor\n",
        "    fit_feature_extractor(feature_extractor, wikitext_train, wikitext_val)\n",
        "\n",
        "    # check test accuracy\n",
        "    test_ppl = calculate_wiki2_test_perplexity(feature_extractor, wikitext_test)\n",
        "\n",
        "    # the real threshold will be released by Oct 11 \n",
        "    assert test_ppl < 10000, 'ummm... your perplexity is too high...'\n",
        "    \n",
        "grade_wikitext2()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss after 0 epoch = 5.0188\n",
            "Validation PPL: 151.2367787907184\n",
            "Test loss = 4.9733\n",
            "Test PPL: 144.50948081333289\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}