{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "class",
      "language": "python",
      "name": "class"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Q1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVdlUH-elw9S",
        "colab_type": "text"
      },
      "source": [
        "---   \n",
        "# HW3 - Transfer learning\n",
        "\n",
        "#### Due October 30, 2019\n",
        "\n",
        "In this assignment you will learn about transfer learning. This technique is perhaps one of the most important techniques for industry. When a problem you want to solve does not have enough data, we use a different (larger) dataset to learn representations which can help us solve our task using the smaller task.\n",
        "\n",
        "The general steps to transfer learning are as follows:\n",
        "\n",
        "1. Find a huge dataset with similar characteristics to the problem you are interested in.\n",
        "2. Choose a model powerful enough to extract meaningful representations from the huge dataset.\n",
        "3. Train this model on the huge dataset.\n",
        "4. Use this model to train on the smaller dataset.\n",
        "\n",
        "\n",
        "### This homework has the following sections:\n",
        "1. Question 1: MNIST fine-tuning (Parts A, B, C, D).\n",
        "2. Question 2: Pretrain on Wikitext2 (Part A, B, C, D)\n",
        "3. Question 3: Finetune on MNLI (Part A, B, C, D)\n",
        "4. Question 4: Finetune using pretrained BERT (Part A, B, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOiT8DAwlw9U",
        "colab_type": "text"
      },
      "source": [
        "---   \n",
        "## Question 1 (MNIST transfer learning)\n",
        "To grasp the high-level approach to transfer learning, let's first do a simple example using computer vision. \n",
        "\n",
        "The torchvision library has pretrained models (resnets, vggnets, etc) on the Imagenet dataset. Imagenet is a dataset\n",
        "with 1.3 million images covering over 1000 classes of objects. When you use one of these models, the weights of the model initialize\n",
        "with the weights saved from training on imagenet.\n",
        "\n",
        "In this task we will:\n",
        "1. Choose a pretrained model.\n",
        "2. Freeze the model so that the weights don't change.\n",
        "3. Fine-tune on a few labels of MNIST.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zQtRyoHlw9V",
        "colab_type": "text"
      },
      "source": [
        "#### Choose a model\n",
        "Here we pick any of the models from torchvision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWinD-2Elw9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "class Identity(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "# init the pretrained feature extractor\n",
        "pretrained_resnet18 = models.resnet18(pretrained=True)\n",
        "#pretrained_resnet18\n",
        "# we don't want the built in last layer, we're going to modify it ourselves\n",
        "#pretrained_resnet18.fc = Identity()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIhTTcbkmtjl",
        "colab_type": "code",
        "outputId": "fd3dd181-c1bb-4362-c203-a3350889daf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pretrained_resnet18.fc = Identity()\n",
        "pretrained_resnet18.fc"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Identity()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZQRbv8SwpNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pretrained_resnet18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQPB1Itblw9b",
        "colab_type": "text"
      },
      "source": [
        "#### Freeze the model\n",
        "Here we freeze the weights of the model. Freezing means the gradients will not backpropagate\n",
        "into these weights.\n",
        "\n",
        "By doing this you can think about the model as a feature extractor. This feature extractor outputs\n",
        "a **representation** of an input. This representation is a matrix that encodes information about the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMc4XRVrlw9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def freeze_model(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "def unfreeze_model(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "        \n",
        "freeze_model(pretrained_resnet18)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdRUwHcPlw9e",
        "colab_type": "text"
      },
      "source": [
        "#### Init target dataset\n",
        "Here we define the dataset we are actually interested in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3poNJjIlw9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import  MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#  train/val  split\n",
        "transform = transforms.Compose([transforms.Grayscale(3),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "mnist_dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
        "mnist_train, mnist_val = random_split(mnist_dataset, [55000, 5000])\n",
        "\n",
        "mnist_train = DataLoader(mnist_train, batch_size=32)\n",
        "mnist_val = DataLoader(mnist_val, batch_size=32)\n",
        "\n",
        "# test split\n",
        "mnist_test = DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transform), batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-aUp_Ma4HXW",
        "colab_type": "code",
        "outputId": "a6b44d0b-fbdc-419d-f0e8-6e94126becc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for images, labels in mnist_train:  \n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    break"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image batch dimensions: torch.Size([32, 3, 28, 28])\n",
            "Image label dimensions: torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK41JTzhlw9h",
        "colab_type": "text"
      },
      "source": [
        "### Part A (init fine-tune model)\n",
        "decide what model to use for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ex7r1ru9rhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQLhCEd1lw9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_fine_tune_model():\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    fine_tune_model = Net(512,256,10)\n",
        "    return fine_tune_model\n",
        "  \n",
        "  \n",
        "#fine_tune_model = init_fine_tune_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp9w-fpHlw9k",
        "colab_type": "text"
      },
      "source": [
        "### Part B (Fine-tune (Frozen))\n",
        "\n",
        "The actual problem we care about solving likely has a different number of classes or is a different task altogether. Fine-tuning is the process of using the extracted representations (features) to solve this downstream task  (the task you're interested in).\n",
        "\n",
        "To illustrate this, we'll use our pretrained model (on Imagenet), to solve the MNIST classification task.\n",
        "\n",
        "There are two types of finetuning. \n",
        "\n",
        "#### 1. Frozen feature_extractor\n",
        "In the first type we pretrain with the FROZEN feature_extractor and NEVER unfreeze it during finetuning.\n",
        "\n",
        "\n",
        "#### 2. Unfrozen feature_extractor\n",
        "In the second, we finetune with a FROZEN feature_extractor for a few epochs, then unfreeze the feature extractor and finish training.\n",
        "\n",
        "\n",
        "In this part we will use the first version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrpWuZE03HRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, unfrozen,patience=5, verbose=False, delta=0,name = 'RNN_'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.name = name\n",
        "        self.unfrozen = unfrozen\n",
        "\n",
        "    def __call__(self, val_loss, model, extractor):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, extractor)\n",
        "        elif score < self.best_score - self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, extractor)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, extractor):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.name+'checkpoint.pt')\n",
        "        if self.unfrozen:\n",
        "          torch.save(extractor.state_dict(), self.name+'extractor_checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "        \n",
        "patience = 5\n",
        "early_stopping = EarlyStopping(unfrozen=False,patience=patience, verbose=True,name=\"MLP_frozen_\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwWDtItDlw9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "plot_cache = []\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "\n",
        "def FROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, mnist_train, mnist_val,num_epochs=10,patience=5):\n",
        "    \"\"\"\n",
        "    model is a feature extractor (resnet).\n",
        "    Create a new model which uses those features to finetune on MNIST\n",
        "    \n",
        "    return the fine_tune model\n",
        "    \"\"\"     \n",
        "    \n",
        "    # INSERT YOUR CODE: (train the fine_tune model using features extracted by feature_extractor)\n",
        "    for epoch_number in range(num_epochs):\n",
        "        avg_loss=0\n",
        "        fine_tune_model.train()\n",
        "        feature_extractor.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(mnist_train):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            inp = feature_extractor(inp)\n",
        "            logits = fine_tune_model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "        #do valid\n",
        "        valid_losses = []\n",
        "        fine_tune_model.eval()\n",
        "        feature_extractor.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(mnist_val):\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                inp = feature_extractor(inp)\n",
        "                logits = fine_tune_model(inp)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item())\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "        plot_cache.append((avg_loss, avg_val_loss))\n",
        "    \n",
        "        early_stopping(avg_val_loss, fine_tune_model,feature_extractor)\n",
        "        torch.save({\n",
        "        'loss_cache': plot_cache\n",
        "        }, './fine_tune_model_frozen.pt')\n",
        "    \n",
        "    \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJXvw10Bz5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fine_tune_model = init_fine_tune_model()\n",
        "fine_tune_model.to(current_device)\n",
        "feature_extractor = pretrained_resnet18.to(current_device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "fine_tune_model_parameters = list(feature_extractor.parameters()) + list(fine_tune_model.parameters())\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, fine_tune_model_parameters))\n",
        "#optimizer = optim.Adam(fine_tune_model_parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaOoclKZDX6e",
        "colab_type": "code",
        "outputId": "ff816d00-4c1a-4854-8d5a-acfdb7bd3875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(np.transpose(images[11],(1,2,0)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa013ec5fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsD1vCcXlw9n",
        "colab_type": "text"
      },
      "source": [
        "### Part C (compute test accuracy)\n",
        "Compute the test accuracy of fine-tuned model on MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jz_MY1xlw9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_mnist_test_accuracy(feature_extractor, fine_tune_model, mnist_test):\n",
        "    \n",
        "    # YOUR CODE HERE...\n",
        "\n",
        "    feature_extractor.eval()\n",
        "    fine_tune_model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for inp,target in mnist_test:\n",
        "      inp = inp.to(current_device)\n",
        "      target = target.to(current_device)\n",
        "      inp = feature_extractor(inp)\n",
        "      logits = fine_tune_model(inp)\n",
        "      outputs = F.softmax(logits, dim=1)\n",
        "      predicted = outputs.max(1, keepdim=True)[1]\n",
        "      temp = predicted\n",
        "      total += target.size(0)\n",
        "      correct += predicted.eq(target.view_as(predicted).to(current_device)).sum().item()\n",
        "    return (100 * correct / total)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUORHRFRnFYp",
        "colab_type": "code",
        "outputId": "ca466b2d-f6ed-415f-e80a-3c0556c8a9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " calculate_mnist_test_accuracy(feature_extractor, fine_tune_model, mnist_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVhwPdAelw9q",
        "colab_type": "text"
      },
      "source": [
        "### Grade!\n",
        "Let's see how you did"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znsdf0UWlw9r",
        "colab_type": "code",
        "outputId": "a55c13f5-99f9-4dd8-ca3e-de72c1146cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def grade_mnist_frozen(load_pretrained):\n",
        "    \n",
        "    # init a ft model\n",
        "    #fine_tune_model = init_fine_tune_model().to(current_device)\n",
        "    \n",
        "    # run the transfer learning routine\n",
        "    if not load_pretrained:\n",
        "      FROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, mnist_train, mnist_val,num_epochs=10,patience=5)\n",
        "    \n",
        "    frozen_model = init_fine_tune_model()\n",
        "    frozen_model.load_state_dict(torch.load(\"MLP_frozen_checkpoint.pt\"))\n",
        "    frozen_model.to(current_device)\n",
        "    # calculate test accuracy\n",
        "    test_accuracy = calculate_mnist_test_accuracy(feature_extractor, frozen_model, mnist_test)\n",
        "    \n",
        "    # the real threshold will be released by Oct 11 \n",
        "    assert test_accuracy > 0.0, 'your accuracy is too low...'\n",
        "    \n",
        "    return test_accuracy\n",
        "    \n",
        "frozen_test_accuracy = grade_mnist_frozen(load_pretrained=False)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 2.3100\n",
            "Step 100 avg train loss = 1.5123\n",
            "Step 200 avg train loss = 1.1640\n",
            "Step 300 avg train loss = 1.0463\n",
            "Step 400 avg train loss = 0.9684\n",
            "Step 500 avg train loss = 0.9852\n",
            "Step 600 avg train loss = 0.9466\n",
            "Step 700 avg train loss = 0.9531\n",
            "Step 800 avg train loss = 0.9319\n",
            "Step 900 avg train loss = 0.8971\n",
            "Step 1000 avg train loss = 0.8928\n",
            "Step 1100 avg train loss = 0.8837\n",
            "Step 1200 avg train loss = 0.8593\n",
            "Step 1300 avg train loss = 0.8264\n",
            "Step 1400 avg train loss = 0.8586\n",
            "Step 1500 avg train loss = 0.8186\n",
            "Step 1600 avg train loss = 0.8034\n",
            "Step 1700 avg train loss = 0.8276\n",
            "Validation loss after 0 epoch = 0.7022\n",
            "Validation loss decreased (inf --> 0.702193).  Saving model ...\n",
            "Step 0 avg train loss = 1.0094\n",
            "Step 100 avg train loss = 0.7951\n",
            "Step 200 avg train loss = 0.8024\n",
            "Step 300 avg train loss = 0.7844\n",
            "Step 400 avg train loss = 0.7438\n",
            "Step 500 avg train loss = 0.7734\n",
            "Step 600 avg train loss = 0.7532\n",
            "Step 700 avg train loss = 0.7785\n",
            "Step 800 avg train loss = 0.7567\n",
            "Step 900 avg train loss = 0.7273\n",
            "Step 1000 avg train loss = 0.7324\n",
            "Step 1100 avg train loss = 0.7297\n",
            "Step 1200 avg train loss = 0.6998\n",
            "Step 1300 avg train loss = 0.6798\n",
            "Step 1400 avg train loss = 0.7047\n",
            "Step 1500 avg train loss = 0.6820\n",
            "Step 1600 avg train loss = 0.6593\n",
            "Step 1700 avg train loss = 0.6826\n",
            "Validation loss after 1 epoch = 0.6647\n",
            "Validation loss decreased (0.702193 --> 0.664744).  Saving model ...\n",
            "Step 0 avg train loss = 0.9007\n",
            "Step 100 avg train loss = 0.6681\n",
            "Step 200 avg train loss = 0.6755\n",
            "Step 300 avg train loss = 0.6753\n",
            "Step 400 avg train loss = 0.6457\n",
            "Step 500 avg train loss = 0.6603\n",
            "Step 600 avg train loss = 0.6380\n",
            "Step 700 avg train loss = 0.6678\n",
            "Step 800 avg train loss = 0.6503\n",
            "Step 900 avg train loss = 0.6282\n",
            "Step 1000 avg train loss = 0.6413\n",
            "Step 1100 avg train loss = 0.6197\n",
            "Step 1200 avg train loss = 0.6021\n",
            "Step 1300 avg train loss = 0.5905\n",
            "Step 1400 avg train loss = 0.6056\n",
            "Step 1500 avg train loss = 0.5857\n",
            "Step 1600 avg train loss = 0.5526\n",
            "Step 1700 avg train loss = 0.5809\n",
            "Validation loss after 2 epoch = 0.6601\n",
            "Validation loss decreased (0.664744 --> 0.660129).  Saving model ...\n",
            "Step 0 avg train loss = 0.8277\n",
            "Step 100 avg train loss = 0.5791\n",
            "Step 200 avg train loss = 0.5774\n",
            "Step 300 avg train loss = 0.5811\n",
            "Step 400 avg train loss = 0.5563\n",
            "Step 500 avg train loss = 0.5826\n",
            "Step 600 avg train loss = 0.5368\n",
            "Step 700 avg train loss = 0.5848\n",
            "Step 800 avg train loss = 0.5627\n",
            "Step 900 avg train loss = 0.5372\n",
            "Step 1000 avg train loss = 0.5587\n",
            "Step 1100 avg train loss = 0.5442\n",
            "Step 1200 avg train loss = 0.5204\n",
            "Step 1300 avg train loss = 0.5080\n",
            "Step 1400 avg train loss = 0.5161\n",
            "Step 1500 avg train loss = 0.5068\n",
            "Step 1600 avg train loss = 0.4826\n",
            "Step 1700 avg train loss = 0.4912\n",
            "Validation loss after 3 epoch = 0.6831\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Step 0 avg train loss = 0.7241\n",
            "Step 100 avg train loss = 0.5023\n",
            "Step 200 avg train loss = 0.5016\n",
            "Step 300 avg train loss = 0.5085\n",
            "Step 400 avg train loss = 0.4786\n",
            "Step 500 avg train loss = 0.5077\n",
            "Step 600 avg train loss = 0.4648\n",
            "Step 700 avg train loss = 0.5044\n",
            "Step 800 avg train loss = 0.4784\n",
            "Step 900 avg train loss = 0.4584\n",
            "Step 1000 avg train loss = 0.4880\n",
            "Step 1100 avg train loss = 0.4724\n",
            "Step 1200 avg train loss = 0.4349\n",
            "Step 1300 avg train loss = 0.4372\n",
            "Step 1400 avg train loss = 0.4463\n",
            "Step 1500 avg train loss = 0.4351\n",
            "Step 1600 avg train loss = 0.4112\n",
            "Step 1700 avg train loss = 0.4278\n",
            "Validation loss after 4 epoch = 0.7647\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Step 0 avg train loss = 0.7669\n",
            "Step 100 avg train loss = 0.4354\n",
            "Step 200 avg train loss = 0.4460\n",
            "Step 300 avg train loss = 0.4379\n",
            "Step 400 avg train loss = 0.4136\n",
            "Step 500 avg train loss = 0.4324\n",
            "Step 600 avg train loss = 0.3984\n",
            "Step 700 avg train loss = 0.4351\n",
            "Step 800 avg train loss = 0.4134\n",
            "Step 900 avg train loss = 0.3994\n",
            "Step 1000 avg train loss = 0.4164\n",
            "Step 1100 avg train loss = 0.3995\n",
            "Step 1200 avg train loss = 0.3763\n",
            "Step 1300 avg train loss = 0.3851\n",
            "Step 1400 avg train loss = 0.3909\n",
            "Step 1500 avg train loss = 0.3759\n",
            "Step 1600 avg train loss = 0.3580\n",
            "Step 1700 avg train loss = 0.3705\n",
            "Validation loss after 5 epoch = 0.8057\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Step 0 avg train loss = 0.5498\n",
            "Step 100 avg train loss = 0.3813\n",
            "Step 200 avg train loss = 0.3879\n",
            "Step 300 avg train loss = 0.3801\n",
            "Step 400 avg train loss = 0.3612\n",
            "Step 500 avg train loss = 0.3823\n",
            "Step 600 avg train loss = 0.3421\n",
            "Step 700 avg train loss = 0.3741\n",
            "Step 800 avg train loss = 0.3630\n",
            "Step 900 avg train loss = 0.3370\n",
            "Step 1000 avg train loss = 0.3543\n",
            "Step 1100 avg train loss = 0.3494\n",
            "Step 1200 avg train loss = 0.3266\n",
            "Step 1300 avg train loss = 0.3397\n",
            "Step 1400 avg train loss = 0.3427\n",
            "Step 1500 avg train loss = 0.3193\n",
            "Step 1600 avg train loss = 0.3159\n",
            "Step 1700 avg train loss = 0.3092\n",
            "Validation loss after 6 epoch = 0.8667\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Step 0 avg train loss = 0.4536\n",
            "Step 100 avg train loss = 0.3243\n",
            "Step 200 avg train loss = 0.3333\n",
            "Step 300 avg train loss = 0.3476\n",
            "Step 400 avg train loss = 0.3223\n",
            "Step 500 avg train loss = 0.3432\n",
            "Step 600 avg train loss = 0.3084\n",
            "Step 700 avg train loss = 0.3145\n",
            "Step 800 avg train loss = 0.3274\n",
            "Step 900 avg train loss = 0.2988\n",
            "Step 1000 avg train loss = 0.2990\n",
            "Step 1100 avg train loss = 0.2956\n",
            "Step 1200 avg train loss = 0.2801\n",
            "Step 1300 avg train loss = 0.2792\n",
            "Step 1400 avg train loss = 0.2830\n",
            "Step 1500 avg train loss = 0.2786\n",
            "Step 1600 avg train loss = 0.2680\n",
            "Step 1700 avg train loss = 0.2663\n",
            "Validation loss after 7 epoch = 0.9626\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2Eosrk91wl1",
        "colab_type": "code",
        "outputId": "450732fd-05be-486d-b283-f4a000fbb366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(frozen_test_accuracy)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bKaa_swlw9t",
        "colab_type": "text"
      },
      "source": [
        "### Part D (Fine-tune Unfrozen)\n",
        "Now we'll learn how to train using the \"unfrozen\" approach.\n",
        "\n",
        "In this approach we'll:\n",
        "1. keep the feature_extract frozen for a few epochs (10)\n",
        "2. Unfreeze it.\n",
        "3. Finish training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOrywSkEFo-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "patience = 5\n",
        "early_stopping_unfrozen = EarlyStopping(unfrozen=True,patience=patience, verbose=True,name=\"MLP_unfrozen_\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27fRXyWSlw9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UNFROZEN_fine_tune_mnist(feature_extractor, fine_tune_model, mnist_train, mnist_val,num_epochs=50,patience=5):\n",
        "    \"\"\"\n",
        "    model is a feature extractor (resnet).\n",
        "    Create a new model which uses those features to finetune on MNIST\n",
        "    \n",
        "    return the fine_tune model\n",
        "    \"\"\"     \n",
        "    \n",
        "    # INSERT YOUR CODE:\n",
        "    # keep frozen for 10 epochs\n",
        "    # ... train\n",
        "    # unfreeze\n",
        "    # train for rest of the time\n",
        "    criterion_unfrozen = nn.CrossEntropyLoss()\n",
        "    fine_tune_model_parameters_unfrozen = list(feature_extractor.parameters()) + list(fine_tune_model.parameters())\n",
        "    optimizer_unfrozen = optim.Adam(filter(lambda p: p.requires_grad, fine_tune_model_parameters_unfrozen))\n",
        "    for epoch_number in range(10):\n",
        "        avg_loss=0\n",
        "        fine_tune_model.train()\n",
        "        feature_extractor.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(mnist_train):\n",
        "            optimizer_unfrozen.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            inp = feature_extractor(inp)\n",
        "            logits = fine_tune_model(inp)\n",
        "            \n",
        "            loss = criterion_unfrozen(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer_unfrozen.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "            \n",
        "        #do valid\n",
        "        valid_losses = []\n",
        "        fine_tune_model.eval()\n",
        "        feature_extractor.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(mnist_val):\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                inp = feature_extractor(inp)\n",
        "                logits = fine_tune_model(inp)\n",
        "\n",
        "                loss = criterion_unfrozen(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item())\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "        plot_cache.append((avg_loss, avg_val_loss))\n",
        "    \n",
        "    unfreeze_model(feature_extractor)\n",
        "    fine_tune_model_parameters_unfrozen2 = list(feature_extractor.parameters()) + list(fine_tune_model.parameters())\n",
        "    optimizer_unfrozen2 = optim.Adam(filter(lambda p: p.requires_grad, fine_tune_model_parameters_unfrozen2))\n",
        "    \n",
        "    \n",
        "    for epoch_number in range(num_epochs):\n",
        "        avg_loss=0\n",
        "        fine_tune_model.train()\n",
        "        feature_extractor.train()\n",
        "        train_log_cache = []\n",
        "        for i, (inp, target) in enumerate(mnist_train):\n",
        "            optimizer_unfrozen2.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            inp = feature_extractor(inp)\n",
        "            logits = fine_tune_model(inp)\n",
        "            \n",
        "            loss = criterion_unfrozen(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer_unfrozen2.step()\n",
        "            \n",
        "            train_log_cache.append(loss.item())\n",
        "            \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "                train_log_cache = []\n",
        "        #do valid\n",
        "        valid_losses = []\n",
        "        fine_tune_model.eval()\n",
        "        feature_extractor.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inp, target) in enumerate(mnist_val):\n",
        "                inp = inp.to(current_device)\n",
        "                target = target.to(current_device)\n",
        "                inp = feature_extractor(inp)\n",
        "                logits = fine_tune_model(inp)\n",
        "\n",
        "                loss = criterion_unfrozen(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "                valid_losses.append(loss.item())\n",
        "            avg_val_loss = sum(valid_losses) / len(valid_losses)\n",
        "            print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "        plot_cache.append((avg_loss, avg_val_loss))\n",
        "                \n",
        "        early_stopping_unfrozen(avg_val_loss, fine_tune_model,feature_extractor)\n",
        "        torch.save({\n",
        "            'loss_cache': plot_cache\n",
        "        }, './fine_tune_model_unfrozen.pt')\n",
        "    \n",
        "    \n",
        "        if early_stopping_unfrozen.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wFWI4mMlw9w",
        "colab_type": "text"
      },
      "source": [
        "### Grade UNFROZEN\n",
        "Let's see if there's a difference in accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooLx29xq2x2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fine_tune_model_unfrozen = init_fine_tune_model()\n",
        "fine_tune_model_unfrozen.to(current_device)\n",
        "feature_extractor_unfrozen = pretrained_resnet18.to(current_device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqU3MQKplw9x",
        "colab_type": "code",
        "outputId": "b25735d5-852c-4da0-a7d1-4a845b9e8c5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def grade_mnist_unfrozen(load_pretrained):\n",
        "    \n",
        "    # init a ft model\n",
        "    #fine_tune_model = init_fine_tune_model()\n",
        "    \n",
        "    # run the transfer learning routine\n",
        "    if not load_pretrained:\n",
        "      UNFROZEN_fine_tune_mnist(feature_extractor_unfrozen,fine_tune_model_unfrozen, mnist_train, mnist_val,num_epochs=20,patience=5)\n",
        "    \n",
        "    # calculate test accuracy\n",
        "    feature_extractor_unfrozen_test = models.resnet18(pretrained=False)\n",
        "    feature_extractor_unfrozen_test.fc = Identity()\n",
        "    feature_extractor_unfrozen_test.load_state_dict(torch.load(\"MLP_unfrozen_extractor_checkpoint.pt\"))\n",
        "    feature_extractor_unfrozen_test.to(current_device)\n",
        "    \n",
        "    fine_tune_model_unfrozen_test = init_fine_tune_model()\n",
        "    fine_tune_model_unfrozen_test.load_state_dict(torch.load(\"MLP_unfrozen_checkpoint.pt\"))\n",
        "    fine_tune_model_unfrozen_test.to(current_device)\n",
        "    test_accuracy = calculate_mnist_test_accuracy(feature_extractor_unfrozen_test, fine_tune_model_unfrozen_test, mnist_test)\n",
        "    \n",
        "    # the real threshold will be released by Oct 11 \n",
        "    assert test_accuracy > 0.0, 'your accuracy is too low...'\n",
        "    \n",
        "    return test_accuracy\n",
        "    \n",
        "unfrozen_test_accuracy = grade_mnist_unfrozen(load_pretrained=False)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 2.4724\n",
            "Step 100 avg train loss = 1.5088\n",
            "Step 200 avg train loss = 1.1572\n",
            "Step 300 avg train loss = 1.0460\n",
            "Step 400 avg train loss = 0.9680\n",
            "Step 500 avg train loss = 0.9905\n",
            "Step 600 avg train loss = 0.9459\n",
            "Step 700 avg train loss = 0.9537\n",
            "Step 800 avg train loss = 0.9300\n",
            "Step 900 avg train loss = 0.8996\n",
            "Step 1000 avg train loss = 0.8848\n",
            "Step 1100 avg train loss = 0.8908\n",
            "Step 1200 avg train loss = 0.8599\n",
            "Step 1300 avg train loss = 0.8338\n",
            "Step 1400 avg train loss = 0.8602\n",
            "Step 1500 avg train loss = 0.8217\n",
            "Step 1600 avg train loss = 0.7981\n",
            "Step 1700 avg train loss = 0.8288\n",
            "Validation loss after 0 epoch = 0.7101\n",
            "Step 0 avg train loss = 1.0025\n",
            "Step 100 avg train loss = 0.7916\n",
            "Step 200 avg train loss = 0.8039\n",
            "Step 300 avg train loss = 0.7874\n",
            "Step 400 avg train loss = 0.7486\n",
            "Step 500 avg train loss = 0.7796\n",
            "Step 600 avg train loss = 0.7441\n",
            "Step 700 avg train loss = 0.7759\n",
            "Step 800 avg train loss = 0.7580\n",
            "Step 900 avg train loss = 0.7223\n",
            "Step 1000 avg train loss = 0.7396\n",
            "Step 1100 avg train loss = 0.7262\n",
            "Step 1200 avg train loss = 0.7162\n",
            "Step 1300 avg train loss = 0.6825\n",
            "Step 1400 avg train loss = 0.7092\n",
            "Step 1500 avg train loss = 0.6768\n",
            "Step 1600 avg train loss = 0.6517\n",
            "Step 1700 avg train loss = 0.6848\n",
            "Validation loss after 1 epoch = 0.6515\n",
            "Step 0 avg train loss = 0.7297\n",
            "Step 100 avg train loss = 0.6624\n",
            "Step 200 avg train loss = 0.6862\n",
            "Step 300 avg train loss = 0.6600\n",
            "Step 400 avg train loss = 0.6515\n",
            "Step 500 avg train loss = 0.6628\n",
            "Step 600 avg train loss = 0.6256\n",
            "Step 700 avg train loss = 0.6651\n",
            "Step 800 avg train loss = 0.6532\n",
            "Step 900 avg train loss = 0.6173\n",
            "Step 1000 avg train loss = 0.6433\n",
            "Step 1100 avg train loss = 0.6163\n",
            "Step 1200 avg train loss = 0.6090\n",
            "Step 1300 avg train loss = 0.5817\n",
            "Step 1400 avg train loss = 0.6024\n",
            "Step 1500 avg train loss = 0.5809\n",
            "Step 1600 avg train loss = 0.5596\n",
            "Step 1700 avg train loss = 0.5811\n",
            "Validation loss after 2 epoch = 0.6452\n",
            "Step 0 avg train loss = 0.5819\n",
            "Step 100 avg train loss = 0.5676\n",
            "Step 200 avg train loss = 0.5883\n",
            "Step 300 avg train loss = 0.5619\n",
            "Step 400 avg train loss = 0.5709\n",
            "Step 500 avg train loss = 0.5825\n",
            "Step 600 avg train loss = 0.5390\n",
            "Step 700 avg train loss = 0.5782\n",
            "Step 800 avg train loss = 0.5669\n",
            "Step 900 avg train loss = 0.5230\n",
            "Step 1000 avg train loss = 0.5553\n",
            "Step 1100 avg train loss = 0.5310\n",
            "Step 1200 avg train loss = 0.5302\n",
            "Step 1300 avg train loss = 0.4995\n",
            "Step 1400 avg train loss = 0.5168\n",
            "Step 1500 avg train loss = 0.4930\n",
            "Step 1600 avg train loss = 0.4744\n",
            "Step 1700 avg train loss = 0.5000\n",
            "Validation loss after 3 epoch = 0.6968\n",
            "Step 0 avg train loss = 0.4323\n",
            "Step 100 avg train loss = 0.4994\n",
            "Step 200 avg train loss = 0.5106\n",
            "Step 300 avg train loss = 0.4840\n",
            "Step 400 avg train loss = 0.4888\n",
            "Step 500 avg train loss = 0.4991\n",
            "Step 600 avg train loss = 0.4581\n",
            "Step 700 avg train loss = 0.4965\n",
            "Step 800 avg train loss = 0.4830\n",
            "Step 900 avg train loss = 0.4510\n",
            "Step 1000 avg train loss = 0.4870\n",
            "Step 1100 avg train loss = 0.4564\n",
            "Step 1200 avg train loss = 0.4529\n",
            "Step 1300 avg train loss = 0.4251\n",
            "Step 1400 avg train loss = 0.4462\n",
            "Step 1500 avg train loss = 0.4212\n",
            "Step 1600 avg train loss = 0.4064\n",
            "Step 1700 avg train loss = 0.4292\n",
            "Validation loss after 4 epoch = 0.7523\n",
            "Step 0 avg train loss = 0.3706\n",
            "Step 100 avg train loss = 0.4390\n",
            "Step 200 avg train loss = 0.4274\n",
            "Step 300 avg train loss = 0.4088\n",
            "Step 400 avg train loss = 0.4226\n",
            "Step 500 avg train loss = 0.4376\n",
            "Step 600 avg train loss = 0.3842\n",
            "Step 700 avg train loss = 0.4257\n",
            "Step 800 avg train loss = 0.4094\n",
            "Step 900 avg train loss = 0.3879\n",
            "Step 1000 avg train loss = 0.4188\n",
            "Step 1100 avg train loss = 0.3882\n",
            "Step 1200 avg train loss = 0.3863\n",
            "Step 1300 avg train loss = 0.3681\n",
            "Step 1400 avg train loss = 0.3736\n",
            "Step 1500 avg train loss = 0.3561\n",
            "Step 1600 avg train loss = 0.3461\n",
            "Step 1700 avg train loss = 0.3683\n",
            "Validation loss after 5 epoch = 0.7897\n",
            "Step 0 avg train loss = 0.3516\n",
            "Step 100 avg train loss = 0.3787\n",
            "Step 200 avg train loss = 0.3772\n",
            "Step 300 avg train loss = 0.3416\n",
            "Step 400 avg train loss = 0.3528\n",
            "Step 500 avg train loss = 0.3852\n",
            "Step 600 avg train loss = 0.3321\n",
            "Step 700 avg train loss = 0.3612\n",
            "Step 800 avg train loss = 0.3542\n",
            "Step 900 avg train loss = 0.3350\n",
            "Step 1000 avg train loss = 0.3715\n",
            "Step 1100 avg train loss = 0.3317\n",
            "Step 1200 avg train loss = 0.3271\n",
            "Step 1300 avg train loss = 0.3094\n",
            "Step 1400 avg train loss = 0.3162\n",
            "Step 1500 avg train loss = 0.3062\n",
            "Step 1600 avg train loss = 0.2937\n",
            "Step 1700 avg train loss = 0.3064\n",
            "Validation loss after 6 epoch = 0.8424\n",
            "Step 0 avg train loss = 0.2604\n",
            "Step 100 avg train loss = 0.3260\n",
            "Step 200 avg train loss = 0.3282\n",
            "Step 300 avg train loss = 0.2984\n",
            "Step 400 avg train loss = 0.3019\n",
            "Step 500 avg train loss = 0.3339\n",
            "Step 600 avg train loss = 0.2800\n",
            "Step 700 avg train loss = 0.3209\n",
            "Step 800 avg train loss = 0.3134\n",
            "Step 900 avg train loss = 0.2759\n",
            "Step 1000 avg train loss = 0.3340\n",
            "Step 1100 avg train loss = 0.2899\n",
            "Step 1200 avg train loss = 0.2857\n",
            "Step 1300 avg train loss = 0.2697\n",
            "Step 1400 avg train loss = 0.2704\n",
            "Step 1500 avg train loss = 0.2658\n",
            "Step 1600 avg train loss = 0.2565\n",
            "Step 1700 avg train loss = 0.2649\n",
            "Validation loss after 7 epoch = 0.9189\n",
            "Step 0 avg train loss = 0.2911\n",
            "Step 100 avg train loss = 0.2870\n",
            "Step 200 avg train loss = 0.2832\n",
            "Step 300 avg train loss = 0.2595\n",
            "Step 400 avg train loss = 0.2558\n",
            "Step 500 avg train loss = 0.2924\n",
            "Step 600 avg train loss = 0.2431\n",
            "Step 700 avg train loss = 0.2929\n",
            "Step 800 avg train loss = 0.2591\n",
            "Step 900 avg train loss = 0.2353\n",
            "Step 1000 avg train loss = 0.2772\n",
            "Step 1100 avg train loss = 0.2457\n",
            "Step 1200 avg train loss = 0.2483\n",
            "Step 1300 avg train loss = 0.2430\n",
            "Step 1400 avg train loss = 0.2453\n",
            "Step 1500 avg train loss = 0.2528\n",
            "Step 1600 avg train loss = 0.2293\n",
            "Step 1700 avg train loss = 0.2466\n",
            "Validation loss after 8 epoch = 1.0003\n",
            "Step 0 avg train loss = 0.1885\n",
            "Step 100 avg train loss = 0.2361\n",
            "Step 200 avg train loss = 0.2493\n",
            "Step 300 avg train loss = 0.2441\n",
            "Step 400 avg train loss = 0.2367\n",
            "Step 500 avg train loss = 0.2595\n",
            "Step 600 avg train loss = 0.2181\n",
            "Step 700 avg train loss = 0.2461\n",
            "Step 800 avg train loss = 0.2415\n",
            "Step 900 avg train loss = 0.2190\n",
            "Step 1000 avg train loss = 0.2480\n",
            "Step 1100 avg train loss = 0.2140\n",
            "Step 1200 avg train loss = 0.2238\n",
            "Step 1300 avg train loss = 0.2135\n",
            "Step 1400 avg train loss = 0.2130\n",
            "Step 1500 avg train loss = 0.2022\n",
            "Step 1600 avg train loss = 0.2075\n",
            "Step 1700 avg train loss = 0.2034\n",
            "Validation loss after 9 epoch = 1.0623\n",
            "Step 0 avg train loss = 0.1921\n",
            "Step 100 avg train loss = 1.2830\n",
            "Step 200 avg train loss = 0.3529\n",
            "Step 300 avg train loss = 0.2975\n",
            "Step 400 avg train loss = 0.2417\n",
            "Step 500 avg train loss = 0.1619\n",
            "Step 600 avg train loss = 0.1827\n",
            "Step 700 avg train loss = 0.1726\n",
            "Step 800 avg train loss = 0.1472\n",
            "Step 900 avg train loss = 0.1408\n",
            "Step 1000 avg train loss = 0.1059\n",
            "Step 1100 avg train loss = 0.0916\n",
            "Step 1200 avg train loss = 0.0947\n",
            "Step 1300 avg train loss = 0.1460\n",
            "Step 1400 avg train loss = 0.1442\n",
            "Step 1500 avg train loss = 0.1144\n",
            "Step 1600 avg train loss = 0.0808\n",
            "Step 1700 avg train loss = 0.1087\n",
            "Validation loss after 0 epoch = 0.1099\n",
            "Validation loss decreased (inf --> 0.109898).  Saving model ...\n",
            "Step 0 avg train loss = 0.3515\n",
            "Step 100 avg train loss = 0.1420\n",
            "Step 200 avg train loss = 0.1245\n",
            "Step 300 avg train loss = 0.0748\n",
            "Step 400 avg train loss = 0.0650\n",
            "Step 500 avg train loss = 0.0816\n",
            "Step 600 avg train loss = 0.0827\n",
            "Step 700 avg train loss = 0.0667\n",
            "Step 800 avg train loss = 0.0756\n",
            "Step 900 avg train loss = 0.0769\n",
            "Step 1000 avg train loss = 0.0713\n",
            "Step 1100 avg train loss = 0.0743\n",
            "Step 1200 avg train loss = 0.0676\n",
            "Step 1300 avg train loss = 0.1622\n",
            "Step 1400 avg train loss = 0.1785\n",
            "Step 1500 avg train loss = 0.1073\n",
            "Step 1600 avg train loss = 0.0827\n",
            "Step 1700 avg train loss = 0.1459\n",
            "Validation loss after 1 epoch = 0.0873\n",
            "Validation loss decreased (0.109898 --> 0.087264).  Saving model ...\n",
            "Step 0 avg train loss = 0.4568\n",
            "Step 100 avg train loss = 0.1073\n",
            "Step 200 avg train loss = 0.1022\n",
            "Step 300 avg train loss = 0.0731\n",
            "Step 400 avg train loss = 0.0616\n",
            "Step 500 avg train loss = 0.0752\n",
            "Step 600 avg train loss = 0.0828\n",
            "Step 700 avg train loss = 0.0736\n",
            "Step 800 avg train loss = 0.0569\n",
            "Step 900 avg train loss = 0.0678\n",
            "Step 1000 avg train loss = 0.0606\n",
            "Step 1100 avg train loss = 0.0724\n",
            "Step 1200 avg train loss = 0.0654\n",
            "Step 1300 avg train loss = 0.0720\n",
            "Step 1400 avg train loss = 0.0630\n",
            "Step 1500 avg train loss = 0.0471\n",
            "Step 1600 avg train loss = 0.0529\n",
            "Step 1700 avg train loss = 0.0730\n",
            "Validation loss after 2 epoch = 0.0457\n",
            "Validation loss decreased (0.087264 --> 0.045687).  Saving model ...\n",
            "Step 0 avg train loss = 0.0108\n",
            "Step 100 avg train loss = 0.0562\n",
            "Step 200 avg train loss = 0.0593\n",
            "Step 300 avg train loss = 0.0468\n",
            "Step 400 avg train loss = 0.0430\n",
            "Step 500 avg train loss = 0.0436\n",
            "Step 600 avg train loss = 0.0662\n",
            "Step 700 avg train loss = 0.0325\n",
            "Step 800 avg train loss = 0.0383\n",
            "Step 900 avg train loss = 0.0549\n",
            "Step 1000 avg train loss = 0.0363\n",
            "Step 1100 avg train loss = 0.0434\n",
            "Step 1200 avg train loss = 0.0551\n",
            "Step 1300 avg train loss = 0.0409\n",
            "Step 1400 avg train loss = 0.0498\n",
            "Step 1500 avg train loss = 0.0489\n",
            "Step 1600 avg train loss = 0.0481\n",
            "Step 1700 avg train loss = 0.0619\n",
            "Validation loss after 3 epoch = 0.0438\n",
            "Validation loss decreased (0.045687 --> 0.043837).  Saving model ...\n",
            "Step 0 avg train loss = 0.1068\n",
            "Step 100 avg train loss = 0.0468\n",
            "Step 200 avg train loss = 0.0556\n",
            "Step 300 avg train loss = 0.0395\n",
            "Step 400 avg train loss = 0.0399\n",
            "Step 500 avg train loss = 0.0316\n",
            "Step 600 avg train loss = 0.0597\n",
            "Step 700 avg train loss = 0.0472\n",
            "Step 800 avg train loss = 0.0355\n",
            "Step 900 avg train loss = 0.0512\n",
            "Step 1000 avg train loss = 0.0310\n",
            "Step 1100 avg train loss = 0.0372\n",
            "Step 1200 avg train loss = 0.0412\n",
            "Step 1300 avg train loss = 0.0512\n",
            "Step 1400 avg train loss = 0.0254\n",
            "Step 1500 avg train loss = 0.0368\n",
            "Step 1600 avg train loss = 0.0351\n",
            "Step 1700 avg train loss = 0.0494\n",
            "Validation loss after 4 epoch = 0.0403\n",
            "Validation loss decreased (0.043837 --> 0.040260).  Saving model ...\n",
            "Step 0 avg train loss = 0.1451\n",
            "Step 100 avg train loss = 0.0472\n",
            "Step 200 avg train loss = 0.0537\n",
            "Step 300 avg train loss = 0.0519\n",
            "Step 400 avg train loss = 0.0580\n",
            "Step 500 avg train loss = 0.0345\n",
            "Step 600 avg train loss = 0.0336\n",
            "Step 700 avg train loss = 0.0405\n",
            "Step 800 avg train loss = 0.0297\n",
            "Step 900 avg train loss = 0.0530\n",
            "Step 1000 avg train loss = 0.0476\n",
            "Step 1100 avg train loss = 0.0392\n",
            "Step 1200 avg train loss = 0.0433\n",
            "Step 1300 avg train loss = 0.0496\n",
            "Step 1400 avg train loss = 0.1368\n",
            "Step 1500 avg train loss = 0.0930\n",
            "Step 1600 avg train loss = 0.0832\n",
            "Step 1700 avg train loss = 0.0681\n",
            "Validation loss after 5 epoch = 0.0365\n",
            "Validation loss decreased (0.040260 --> 0.036529).  Saving model ...\n",
            "Step 0 avg train loss = 0.0300\n",
            "Step 100 avg train loss = 0.0929\n",
            "Step 200 avg train loss = 0.0557\n",
            "Step 300 avg train loss = 0.0421\n",
            "Step 400 avg train loss = 0.0327\n",
            "Step 500 avg train loss = 0.0268\n",
            "Step 600 avg train loss = 0.0280\n",
            "Step 700 avg train loss = 0.0204\n",
            "Step 800 avg train loss = 0.0220\n",
            "Step 900 avg train loss = 0.0267\n",
            "Step 1000 avg train loss = 0.0207\n",
            "Step 1100 avg train loss = 0.0239\n",
            "Step 1200 avg train loss = 0.0240\n",
            "Step 1300 avg train loss = 0.0307\n",
            "Step 1400 avg train loss = 0.0334\n",
            "Step 1500 avg train loss = 0.0351\n",
            "Step 1600 avg train loss = 0.0193\n",
            "Step 1700 avg train loss = 0.0140\n",
            "Validation loss after 6 epoch = 0.0387\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Step 0 avg train loss = 0.0057\n",
            "Step 100 avg train loss = 0.0312\n",
            "Step 200 avg train loss = 0.0352\n",
            "Step 300 avg train loss = 0.0233\n",
            "Step 400 avg train loss = 0.0135\n",
            "Step 500 avg train loss = 0.0099\n",
            "Step 600 avg train loss = 0.0415\n",
            "Step 700 avg train loss = 0.0176\n",
            "Step 800 avg train loss = 0.0177\n",
            "Step 900 avg train loss = 0.0302\n",
            "Step 1000 avg train loss = 0.0196\n",
            "Step 1100 avg train loss = 0.0151\n",
            "Step 1200 avg train loss = 0.0223\n",
            "Step 1300 avg train loss = 0.0291\n",
            "Step 1400 avg train loss = 0.0270\n",
            "Step 1500 avg train loss = 0.0341\n",
            "Step 1600 avg train loss = 0.0236\n",
            "Step 1700 avg train loss = 0.0304\n",
            "Validation loss after 7 epoch = 0.0432\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Step 0 avg train loss = 0.0972\n",
            "Step 100 avg train loss = 0.0686\n",
            "Step 200 avg train loss = 0.0961\n",
            "Step 300 avg train loss = 0.0600\n",
            "Step 400 avg train loss = 0.0317\n",
            "Step 500 avg train loss = 0.0187\n",
            "Step 600 avg train loss = 0.0418\n",
            "Step 700 avg train loss = 0.0240\n",
            "Step 800 avg train loss = 0.0203\n",
            "Step 900 avg train loss = 0.0256\n",
            "Step 1000 avg train loss = 0.0177\n",
            "Step 1100 avg train loss = 0.0129\n",
            "Step 1200 avg train loss = 0.0172\n",
            "Step 1300 avg train loss = 0.0345\n",
            "Step 1400 avg train loss = 0.0342\n",
            "Step 1500 avg train loss = 0.0304\n",
            "Step 1600 avg train loss = 0.0327\n",
            "Step 1700 avg train loss = 0.0282\n",
            "Validation loss after 8 epoch = 0.0485\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Step 0 avg train loss = 0.0218\n",
            "Step 100 avg train loss = 0.1277\n",
            "Step 200 avg train loss = 0.0888\n",
            "Step 300 avg train loss = 0.1102\n",
            "Step 400 avg train loss = 0.0463\n",
            "Step 500 avg train loss = 0.0331\n",
            "Step 600 avg train loss = 0.0503\n",
            "Step 700 avg train loss = 0.0204\n",
            "Step 800 avg train loss = 0.0178\n",
            "Step 900 avg train loss = 0.0295\n",
            "Step 1000 avg train loss = 0.0187\n",
            "Step 1100 avg train loss = 0.0180\n",
            "Step 1200 avg train loss = 0.0141\n",
            "Step 1300 avg train loss = 0.0222\n",
            "Step 1400 avg train loss = 0.0401\n",
            "Step 1500 avg train loss = 0.0234\n",
            "Step 1600 avg train loss = 0.0157\n",
            "Step 1700 avg train loss = 0.0140\n",
            "Validation loss after 9 epoch = 0.0490\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Step 0 avg train loss = 0.1740\n",
            "Step 100 avg train loss = 0.0237\n",
            "Step 200 avg train loss = 0.0300\n",
            "Step 300 avg train loss = 0.0229\n",
            "Step 400 avg train loss = 0.0173\n",
            "Step 500 avg train loss = 0.0128\n",
            "Step 600 avg train loss = 0.0139\n",
            "Step 700 avg train loss = 0.0053\n",
            "Step 800 avg train loss = 0.0108\n",
            "Step 900 avg train loss = 0.0202\n",
            "Step 1000 avg train loss = 0.0145\n",
            "Step 1100 avg train loss = 0.0192\n",
            "Step 1200 avg train loss = 0.0125\n",
            "Step 1300 avg train loss = 0.0122\n",
            "Step 1400 avg train loss = 0.0103\n",
            "Step 1500 avg train loss = 0.0184\n",
            "Step 1600 avg train loss = 0.0108\n",
            "Step 1700 avg train loss = 0.0085\n",
            "Validation loss after 10 epoch = 0.0816\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36K_NtZbC5H8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_extractor_unfrozen_test = models.resnet18(pretrained=False)\n",
        "feature_extractor_unfrozen_test.fc = Identity()\n",
        "feature_extractor_unfrozen_test.load_state_dict(torch.load(\"MLP_unfrozen_extractor_checkpoint.pt\"))\n",
        "feature_extractor_unfrozen_test.to(current_device)\n",
        "    \n",
        "fine_tune_model_unfrozen_test = init_fine_tune_model()\n",
        "fine_tune_model_unfrozen_test.load_state_dict(torch.load(\"MLP_unfrozen_checkpoint.pt\"))\n",
        "fine_tune_model_unfrozen_test.to(current_device)\n",
        "unfrozen_test_accuracy = calculate_mnist_test_accuracy(feature_extractor_unfrozen_test, fine_tune_model_unfrozen_test, mnist_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UouaCMHBDCyb",
        "colab_type": "code",
        "outputId": "de31301c-60d0-4e2e-e804-8b4ce40115d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unfrozen_test_accuracy"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.04"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6wMyT8flw9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert unfrozen_test_accuracy > frozen_test_accuracy, 'the unfrozen model should be better'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iimlo_eRDIu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}