{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Q4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5qppgJHnAZm",
        "colab_type": "text"
      },
      "source": [
        "---   \n",
        "# HW3 - Transfer learning\n",
        "\n",
        "#### Due October 30, 2019\n",
        "\n",
        "In this assignment you will learn about transfer learning. This technique is perhaps one of the most important techniques for industry. When a problem you want to solve does not have enough data, we use a different (larger) dataset to learn representations which can help us solve our task using the smaller task.\n",
        "\n",
        "The general steps to transfer learning are as follows:\n",
        "\n",
        "1. Find a huge dataset with similar characteristics to the problem you are interested in.\n",
        "2. Choose a model powerful enough to extract meaningful representations from the huge dataset.\n",
        "3. Train this model on the huge dataset.\n",
        "4. Use this model to train on the smaller dataset.\n",
        "\n",
        "\n",
        "### This homework has the following sections:\n",
        "1. Question 1: MNIST fine-tuning (Parts A, B, C, D).\n",
        "2. Question 2: Pretrain on Wikitext2 (Part A, B, C, D)\n",
        "3. Question 3: Finetune on MNLI (Part A, B, C, D)\n",
        "4. Question 4: Finetune using pretrained BERT (Part A, B, C)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxCd8ddo9AP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "25d31c06-9e47-46eb-9dd1-a6da9e1bd694"
      },
      "source": [
        "!pip install jsonlines\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import  MNIST\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torch import nn\n",
        "import numpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBFAZhfY9BpI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "e0d267b9-ad2a-4ea6-bc91-3ad329ee0bc8"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers.data.processors.glue import MnliProcessor\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import argparse\n",
        "import tempfile\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    BertTokenizer\n",
        ")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/f9/51824e40f0a23a49eab4fcaa45c1c797cbf9761adedd0b558dab7c958b34/transformers-2.1.1-py3-none-any.whl (311kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.16.5)\n",
            "Collecting regex (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 48.4MB/s \n",
            "\u001b[?25hCollecting sacremoses (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 43.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.9.250)\n",
            "Collecting sentencepiece (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 32.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.250 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.12.250)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.250->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.250->boto3->transformers) (2.5.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=8ea7cf3b11204bbb41df0dd967646207bfc9db01e140d72837258d75a11c3187\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: regex, sacremoses, sentencepiece, transformers\n",
            "Successfully installed regex-2019.8.19 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 5422322.16B/s]\n",
            "100%|██████████| 313/313 [00:00<00:00, 153545.87B/s]\n",
            "100%|██████████| 435779157/435779157 [00:08<00:00, 49019016.69B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezAHE7LN9BrF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bad5f2aa-b905-471d-c42e-c27df26758d7"
      },
      "source": [
        "TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"SNLI\", \"QNLI\", \"RTE\", \"WNLI\", \"diagnostic\"]\n",
        "TASK2PATH = {\n",
        "    \"CoLA\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\",  # noqa\n",
        "    \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",  # noqa\n",
        "    \"MRPC\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\",  # noqa\n",
        "    \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP-clean.zip?alt=media&token=11a647cb-ecd3-49c9-9d31-79f8ca8fe277\",  # noqa\n",
        "    \"STS\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\",  # noqa\n",
        "    \"MNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\",  # noqa\n",
        "    \"SNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\",  # noqa\n",
        "    \"QNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\",  # noqa\n",
        "    \"RTE\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\",  # noqa\n",
        "    \"WNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\",  # noqa\n",
        "    \"diagnostic\": [\n",
        "        \"https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D\",  # noqa\n",
        "        \"https://www.dropbox.com/s/ju7d95ifb072q9f/diagnostic-full.tsv?dl=1\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n",
        "MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n",
        "\n",
        "\n",
        "def download_and_extract(task, data_dir):\n",
        "    print(\"Downloading and extracting %s...\" % task)\n",
        "    data_file = \"%s.zip\" % task\n",
        "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
        "    with zipfile.ZipFile(data_file) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(data_file)\n",
        "    print(\"\\tCompleted!\")\n",
        "download_and_extract('MNLI', '.')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting MNLI...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiTJAlibnAbf",
        "colab_type": "text"
      },
      "source": [
        "---  \n",
        "### Question 4 (BERT)\n",
        "\n",
        "A major direction in research came from a model called BERT, released last year.  \n",
        "\n",
        "In this question you'll use BERT as your feature_extractor instead of the model you\n",
        "designed yourself.\n",
        "\n",
        "To get BERT, head on over to (https://github.com/huggingface/transformers) and load your BERT model here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjoTfPfknAbh",
        "colab_type": "text"
      },
      "source": [
        "### Part A (init BERT)\n",
        "In this section you need to create an instance of BERT and return if from the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHqGHbZL9YgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_mnli_dataset():\n",
        "  # ----------------------\n",
        "  # TRAIN/VAL DATALOADERS\n",
        "  # ----------------------\n",
        "  train = processor.get_train_examples('MNLI')\n",
        "  features = convert_examples_to_features(train,\n",
        "                                          tokenizer,\n",
        "                                          label_list=['contradiction','neutral','entailment'],\n",
        "                                          max_length=128,\n",
        "                                          output_mode='classification',\n",
        "                                          pad_on_left=False,\n",
        "                                          pad_token=tokenizer.pad_token_id,\n",
        "                                          pad_token_segment_id=0)\n",
        "  train_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.label for f in features], dtype=torch.long))\n",
        "\n",
        "  nb_train_samples = int(0.95 * len(train_dataset))\n",
        "  nb_val_samples = len(train_dataset) - nb_train_samples\n",
        "\n",
        "  bert_mnli_train_dataset, bert_mnli_val_dataset = random_split(train_dataset, [nb_train_samples, nb_val_samples])\n",
        "\n",
        "  # train loader\n",
        "  train_sampler = RandomSampler(bert_mnli_train_dataset)\n",
        "  bert_mnli_train_dataloader = DataLoader(bert_mnli_train_dataset, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "  # val loader\n",
        "  val_sampler = RandomSampler(bert_mnli_val_dataset)\n",
        "  bert_mnli_val_dataloader = DataLoader(bert_mnli_val_dataset, sampler=val_sampler, batch_size=32)\n",
        "\n",
        "\n",
        "  # ----------------------\n",
        "  # TEST DATALOADERS\n",
        "  # ----------------------\n",
        "  dev = processor.get_dev_examples('MNLI')\n",
        "  features = convert_examples_to_features(dev,\n",
        "                                          tokenizer,\n",
        "                                          label_list=['contradiction','neutral','entailment'],\n",
        "                                          max_length=128,\n",
        "                                          output_mode='classification',\n",
        "                                          pad_on_left=False,\n",
        "                                          pad_token=tokenizer.pad_token_id,\n",
        "                                          pad_token_segment_id=0)\n",
        "\n",
        "  bert_mnli_test_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.label for f in features], dtype=torch.long))\n",
        "\n",
        "  # test dataset\n",
        "  test_sampler = RandomSampler(bert_mnli_test_dataset)\n",
        "  bert_mnli_test_dataloader = DataLoader(bert_mnli_test_dataset, sampler=test_sampler, batch_size=32)\n",
        "  \n",
        "  return bert_mnli_train_dataloader, bert_mnli_val_dataloader, bert_mnli_test_dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSsPsGHg9zux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = MnliProcessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGvcbS_A9oQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader, val_loader, test_loader = init_mnli_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGRTYF29nAbi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a7d74e01-6034-4bef-d61e-bc2bc3fb0cb2"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "def init_bert():\n",
        "    pretrained_weights = \"bert-base-uncased\"\n",
        "    bert = BertModel.from_pretrained(pretrained_weights, output_attentions=True)\n",
        "    def freeze_model(model):\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "    freeze_model(bert)\n",
        "    \n",
        "    return bert\n",
        "  \n",
        "bert = init_bert()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:00<00:00, 174298.61B/s]\n",
            "100%|██████████| 440473133/440473133 [00:07<00:00, 58461152.18B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hvQhENInAbk",
        "colab_type": "text"
      },
      "source": [
        "## Part B (fine-tune with BERT)\n",
        "\n",
        "Use BERT as your feature extractor to finetune MNLI. Use a new finetune model (reset weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRLvAKrE_Zb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTSequenceClassifier(nn.Module):\n",
        "    def __init__(self, bert, num_classes):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.W = nn.Linear(bert.config.hidden_size, num_classes)\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        h, _, attn = self.bert(input_ids=input_ids, \n",
        "                               attention_mask=attention_mask, \n",
        "                               token_type_ids=token_type_ids)\n",
        "        h_cls = h[:, 0]\n",
        "        logits = self.W(h_cls)\n",
        "        return logits, attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlloI7cV_ce_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_finetune_model(bert):\n",
        "  model = BERTSequenceClassifier(bert, 3)\n",
        "  return model\n",
        "\n",
        "fine_tune_model = init_finetune_model(bert)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu-0CiQf_chN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzI7kaq3_cj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_cache = []\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl1_0eiX_ymX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fine_tune_model.to(current_device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.convert_tokens_to_ids(\"[PAD]\")).to(current_device)\n",
        "optimizer = optim.Adam([p for p in fine_tune_model.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwUf3a34UV-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9f2c1b6e-b3fc-4551-c501-ae7ea65f20c1"
      },
      "source": [
        "[p for p in fine_tune_model.parameters() if p.requires_grad]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0237, -0.0244,  0.0062,  ..., -0.0012, -0.0132, -0.0165],\n",
              "         [-0.0315, -0.0306,  0.0277,  ...,  0.0099,  0.0013, -0.0290],\n",
              "         [-0.0132,  0.0326,  0.0145,  ...,  0.0224, -0.0180,  0.0262]],\n",
              "        device='cuda:0', requires_grad=True), Parameter containing:\n",
              " tensor([-0.0111, -0.0088, -0.0087], device='cuda:0', requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xsn6roW_ypA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOAD_PRETRAINED = False\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzPxkXNOnAbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fine_tune_mnli_BERT(model, train_loader, val_loader):\n",
        "  plot_cache = []\n",
        "  for epoch_number in range(2):\n",
        "      avg_loss=0\n",
        "      model.train()\n",
        "      train_log_cache = []\n",
        "      for i, (inp, attention_masks , token_type_ids ,target) in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "          inp = inp.to(current_device)\n",
        "          target = target.to(current_device)\n",
        "          token_type_ids = token_type_ids.to(current_device)\n",
        "          attention_masks = attention_masks.to(current_device)\n",
        "          logits, _ = model(inp,attention_masks,token_type_ids)\n",
        "\n",
        "          loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "\n",
        "          loss.backward()\n",
        "          nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "          train_log_cache.append(loss.item())\n",
        "\n",
        "          if i % 1000 == 0:\n",
        "              avg_loss = sum(train_log_cache)/len(train_log_cache)\n",
        "              print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "              train_log_cache = []\n",
        "\n",
        "      #do valid\n",
        "      valid_losses = []\n",
        "      #model.eval()\n",
        "      with torch.no_grad():\n",
        "          model.eval()\n",
        "          total = 0\n",
        "          correct = 0\n",
        "          for i,(inp, attention_masks , token_type_ids ,target) in enumerate(val_loader):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            token_type_ids = token_type_ids.to(current_device)\n",
        "            attention_masks = attention_masks.to(current_device)\n",
        "            logits, _ = model(inp,attention_masks,token_type_ids)\n",
        "\n",
        "            outputs = F.softmax(logits, dim=1)\n",
        "            predicted = outputs.max(1, keepdim=True)[1]\n",
        "            temp = predicted\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target.view_as(predicted).to(current_device)).sum().item()\n",
        "          print(\"val acc\",100 * correct / total)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9YCZB3eB1vf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "286ffef7-c014-43ec-8e2a-e1e8945fd902"
      },
      "source": [
        "fine_tune_mnli_BERT(fine_tune_model, train_loader, val_loader)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 1.1723\n",
            "Step 1000 avg train loss = 0.7608\n",
            "Step 2000 avg train loss = 0.6769\n",
            "Step 3000 avg train loss = 0.6647\n",
            "Step 4000 avg train loss = 0.6653\n",
            "Step 5000 avg train loss = 0.6591\n",
            "Step 6000 avg train loss = 0.6563\n",
            "Step 7000 avg train loss = 0.6538\n",
            "Step 8000 avg train loss = 0.6515\n",
            "Step 9000 avg train loss = 0.6536\n",
            "Step 10000 avg train loss = 0.6503\n",
            "Step 11000 avg train loss = 0.6491\n",
            "val acc 42.32022815237319\n",
            "Step 0 avg train loss = 0.5556\n",
            "Step 1000 avg train loss = 0.6466\n",
            "Step 2000 avg train loss = 0.6476\n",
            "Step 3000 avg train loss = 0.6466\n",
            "Step 4000 avg train loss = 0.6451\n",
            "Step 5000 avg train loss = 0.6442\n",
            "Step 6000 avg train loss = 0.6457\n",
            "Step 7000 avg train loss = 0.6434\n",
            "Step 8000 avg train loss = 0.6431\n",
            "Step 9000 avg train loss = 0.6440\n",
            "Step 10000 avg train loss = 0.6449\n",
            "Step 11000 avg train loss = 0.6435\n",
            "val acc 43.18598492564677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_xhdZvtnAbn",
        "colab_type": "text"
      },
      "source": [
        "## Part C\n",
        "Evaluate how well we did"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL4GEAG4nAbo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "61feab9c-de65-4b07-d79b-431eeffffbe3"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def calculate_mnli_test_accuracy_BERT(model, test_loader):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i,(inp, attention_masks , token_type_ids ,target) in enumerate(test_loader):\n",
        "      inp = inp.to(current_device)\n",
        "      target = target.to(current_device)\n",
        "      token_type_ids = token_type_ids.to(current_device)\n",
        "      attention_masks = attention_masks.to(current_device)\n",
        "      logits, _ = model(inp,attention_masks,token_type_ids)\n",
        "\n",
        "      outputs = F.softmax(logits, dim=1)\n",
        "      predicted = outputs.max(1, keepdim=True)[1]\n",
        "      temp = predicted\n",
        "      total += target.size(0)\n",
        "      correct += predicted.eq(target.view_as(predicted).to(current_device)).sum().item()\n",
        "    print(\"test acc\",100 * correct / total)\n",
        "  return 100 * correct / total\n",
        "\n",
        "calculate_mnli_test_accuracy_BERT(fine_tune_model, test_loader)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test acc 42.83239938869078\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42.83239938869078"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOdLV9sdnAbp",
        "colab_type": "text"
      },
      "source": [
        "## Let's grade your BERT results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lWOlH9enAbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grade_mnli_BERT():\n",
        "    BERT_feature_extractor = init_bert()\n",
        "    \n",
        "    # load data\n",
        "    #mnli_train, mnli_val, mnli_test = init_mnli_dataset()\n",
        "\n",
        "    # init the fine_tune model\n",
        "    fine_tune_model = init_finetune_model()\n",
        "    \n",
        "    # finetune\n",
        "    fine_tune_mnli(BERT_feature_extractor, fine_tune_model, mnli_train, mnli_val)\n",
        "\n",
        "    # check test accuracy\n",
        "    test_accuracy = calculate_mnli_test_accuracy(feature_extractor, wikitext_test)\n",
        "    \n",
        "    # the real threshold will be released by Oct 11 \n",
        "    assert test_accuracy > 0.0, 'ummm... your accuracy is too low...'\n",
        "    \n",
        "grade_mnli_BERT()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}